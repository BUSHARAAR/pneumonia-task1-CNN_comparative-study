# -*- coding: utf-8 -*-
"""Pneumonia_CNN_VLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9B6f1uv9LSW6YR2TmvKgKb_KGmx_geY
"""

!pip -q install medmnist timm torchmetrics scikit-learn matplotlib

import os, json, math, random, time
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve
)

SEED = 42
def seed_all(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
seed_all()

device = "cuda" if torch.cuda.is_available() else "cpu"
device

import medmnist
from medmnist import PneumoniaMNIST

DATA_ROOT = "./data"
Path(DATA_ROOT).mkdir(parents=True, exist_ok=True)

train_raw = PneumoniaMNIST(split="train", download=True, root=DATA_ROOT)
val_raw   = PneumoniaMNIST(split="val",   download=True, root=DATA_ROOT)
test_raw  = PneumoniaMNIST(split="test",  download=True, root=DATA_ROOT)

len(train_raw), len(val_raw), len(test_raw)

def compute_mean_std_medmnist(ds, max_items=None):
    # ds returns PIL image, label ndarray
    n = len(ds) if max_items is None else min(len(ds), max_items)
    s1 = 0.0
    s2 = 0.0
    cnt = 0
    for i in range(n):
        img, _ = ds[i]
        arr = np.asarray(img, dtype=np.float32) / 255.0  # HxW grayscale
        s1 += arr.mean()
        s2 += (arr**2).mean()
        cnt += 1
    mean = s1 / cnt
    var = (s2 / cnt) - mean**2
    std = float(np.sqrt(max(var, 1e-12)))
    return float(mean), float(std)

train_mean, train_std = compute_mean_std_medmnist(train_raw)
train_mean, train_std

"""2) Dataset wrapper with medical-meaningful augmentation

This keeps CXR-safe augmentations:

✅ slight brightness/contrast

✅ tiny rotation (fixes your earlier “comment vs code” mismatch)

✅ small translation

✅ mild Gaussian noise
(No flips.)
"""



import torchvision.transforms.functional as TF

class Pneumo28(Dataset):
    def __init__(self, base_ds, mean, std, augment=False):
        self.ds = base_ds
        self.mean = mean
        self.std = std
        self.augment = augment

    def __len__(self):
        return len(self.ds)

    def _augment(self, x):
        # x is torch float [1, H, W] in [0,1]
        # brightness/contrast
        if random.random() < 0.8:
            b = random.uniform(0.85, 1.15)
            c = random.uniform(0.85, 1.15)
            x = TF.adjust_brightness(x, b)
            x = TF.adjust_contrast(x, c)

        # small rotation (±7°)
        if random.random() < 0.5:
            angle = random.uniform(-7, 7)
            x = TF.rotate(x, angle=angle)

        # small translation (up to 2 px for 28x28)
        if random.random() < 0.5:
            dx = random.randint(-2, 2)
            dy = random.randint(-2, 2)
            x = TF.affine(x, angle=0, translate=[dx, dy], scale=1.0, shear=[0.0, 0.0])

        # mild gaussian noise
        if random.random() < 0.5:
            x = torch.clamp(x + 0.02 * torch.randn_like(x), 0.0, 1.0)

        return x

    def __getitem__(self, idx):
        img, label = self.ds[idx]
        x = torch.from_numpy(np.asarray(img, dtype=np.float32)).unsqueeze(0) / 255.0  # [1,28,28]
        y = int(np.asarray(label).squeeze())

        if self.augment:
            x = self._augment(x)

        x = (x - self.mean) / (self.std + 1e-8)
        return x, y

train_ds = Pneumo28(train_raw, train_mean, train_std, augment=True)
val_ds   = Pneumo28(val_raw,   train_mean, train_std, augment=False)
test_ds  = Pneumo28(test_raw,  train_mean, train_std, augment=False)

BATCH = 256
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)

"""Model zoo: SimpleCNN + ResNet18 + EfficientNet + ViT"""

import timm

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
        )
        self.head = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.net(x).flatten(1)
        return self.head(x)

def build_model(name="resnet18", num_classes=2):
    name = name.lower()
    if name == "simplecnn":
        return SimpleCNN(num_classes=num_classes)

    if name == "resnet18":
        m = timm.create_model("resnet18", pretrained=True, num_classes=num_classes, in_chans=1)
        return m

    if name == "efficientnet_b0":
        m = timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes, in_chans=1)
        return m

    if name == "vit_tiny":
        # ViT expects larger images; we will upsample to 224 in training/eval if using ViT.
        m = timm.create_model("vit_tiny_patch16_224", pretrained=True, num_classes=num_classes, in_chans=1)
        return m

    raise ValueError(f"Unknown model: {name}")

# quick test
model = build_model("resnet18").to(device)
x, y = next(iter(train_loader))
with torch.no_grad():
    out = model(x.to(device))
out.shape

"""4) Training utilities (AUC, curves, save best weights)"""

def maybe_resize_for_model(x, model_name):
    # ViT needs 224x224; others fine at 28
    if model_name.lower().startswith("vit"):
        x = torch.nn.functional.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)
    return x

@torch.no_grad()
def predict_probs(model, loader, model_name):
    model.eval()
    all_probs, all_y = [], []
    for x, y in loader:
        x = maybe_resize_for_model(x.to(device), model_name)
        logits = model(x)
        probs = torch.softmax(logits, dim=1)[:, 1]
        all_probs.append(probs.cpu().numpy())
        all_y.append(y.numpy())
    return np.concatenate(all_probs), np.concatenate(all_y)

def train_one_epoch(model, loader, optimizer, criterion, model_name):
    model.train()
    running = 0.0
    for x, y in loader:
        x = maybe_resize_for_model(x.to(device), model_name)
        y = y.to(device)

        optimizer.zero_grad(set_to_none=True)
        logits = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        running += loss.item() * y.size(0)
    return running / len(loader.dataset)

@torch.no_grad()
def eval_loss_auc(model, loader, criterion, model_name):
    model.eval()
    running = 0.0
    probs, ys = [], []
    for x, y in loader:
        x = maybe_resize_for_model(x.to(device), model_name)
        y_t = y.to(device)
        logits = model(x)
        loss = criterion(logits, y_t)
        running += loss.item() * y_t.size(0)
        p = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()
        probs.append(p); ys.append(y.numpy())
    probs = np.concatenate(probs)
    ys = np.concatenate(ys)
    auc = roc_auc_score(ys, probs)
    return running / len(loader.dataset), auc

"""5) Train (choose model + hyperparameters)"""

MODEL_NAME = "efficientnet_b0"  # try: "simplecnn", "resnet18", "efficientnet_b0", "vit_tiny"
EPOCHS = 15
LR = 3e-4
WD = 1e-4

model = build_model(MODEL_NAME).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

history = {"train_loss": [], "val_loss": [], "val_auc": []}
best_auc = -1.0
best_path = Path("models")
best_path.mkdir(exist_ok=True)
best_ckpt = best_path / f"best_{MODEL_NAME}.pt"

for epoch in range(1, EPOCHS+1):
    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion, MODEL_NAME)
    va_loss, va_auc = eval_loss_auc(model, val_loader, criterion, MODEL_NAME)
    scheduler.step()

    history["train_loss"].append(tr_loss)
    history["val_loss"].append(va_loss)
    history["val_auc"].append(va_auc)

    if va_auc > best_auc:
        best_auc = va_auc
        torch.save({
            "model_name": MODEL_NAME,
            "state_dict": model.state_dict(),
            "mean": train_mean,
            "std": train_std,
            "epoch": epoch,
            "val_auc": va_auc,
            "hyperparams": {"epochs": EPOCHS, "lr": LR, "wd": WD, "batch": BATCH, "seed": SEED}
        }, best_ckpt)

    print(f"Epoch {epoch:02d} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_auc={va_auc:.4f} | best={best_auc:.4f}")

best_ckpt.as_posix()

Path("outputs").mkdir(exist_ok=True)

plt.figure()
plt.plot(history["train_loss"], label="train_loss")
plt.plot(history["val_loss"], label="val_loss")
plt.legend()
plt.title("Training/Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.savefig("outputs/training_curves_loss.png", dpi=200)
plt.show()

plt.figure()
plt.plot(history["val_auc"], label="val_auc")
plt.legend()
plt.title("Validation AUC")
plt.xlabel("Epoch")
plt.ylabel("AUC")
plt.savefig("outputs/training_curves_auc.png", dpi=200)
plt.show()

"""7) Evaluate on TEST (metrics + confusion matrix + ROC)"""

ckpt = torch.load(best_ckpt, map_location=device, weights_only=False)

MODEL_NAME = ckpt["model_name"]
model = build_model(MODEL_NAME).to(device)
model.load_state_dict(ckpt["state_dict"])

test_probs, test_y = predict_probs(model, test_loader, MODEL_NAME)
test_pred = (test_probs >= 0.5).astype(int)

acc  = accuracy_score(test_y, test_pred)
prec = precision_score(test_y, test_pred, zero_division=0)
rec  = recall_score(test_y, test_pred, zero_division=0)
f1   = f1_score(test_y, test_pred, zero_division=0)
auc  = roc_auc_score(test_y, test_probs)

acc, prec, rec, f1, auc

cm = confusion_matrix(test_y, test_pred)
cm

plt.figure()
plt.imshow(cm)
plt.title("Confusion Matrix (Test)")
plt.xlabel("Predicted")
plt.ylabel("True")
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, str(cm[i, j]), ha="center", va="center")
plt.xticks([0,1], ["Normal(0)", "Pneumonia(1)"])
plt.yticks([0,1], ["Normal(0)", "Pneumonia(1)"])
plt.savefig("outputs/confusion_matrix.png", dpi=200)
plt.show()

fpr, tpr, thr = roc_curve(test_y, test_probs)
plt.figure()
plt.plot(fpr, tpr)
plt.plot([0,1],[0,1], linestyle="--")
plt.title("ROC Curve (Test)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.savefig("outputs/roc_curve.png", dpi=200)
plt.show()

"""8) Failure cases (save misclassified images)"""

def denorm(x, mean, std):
    return (x * (std + 1e-8) + mean).clamp(0, 1)

mis_idx = np.where(test_pred != test_y)[0]
len(mis_idx)

Path("outputs/failures").mkdir(parents=True, exist_ok=True)

# pull raw tensors from test_ds (already normalized); we denorm for visualization
MAX_SHOW = 25
pick = mis_idx[:MAX_SHOW]

plt.figure(figsize=(10,10))
for k, idx in enumerate(pick):
    x, y = test_ds[idx]
    x_vis = denorm(x, train_mean, train_std).squeeze(0).numpy()

    prob = float(test_probs[idx])
    pred = int(test_pred[idx])

    plt.subplot(5, 5, k+1)
    plt.imshow(x_vis, cmap="gray")
    plt.axis("off")
    plt.title(f"T={y} P={pred}\nPr={prob:.2f}", fontsize=8)

    plt.imsave(f"outputs/failures/failure_{idx}_T{y}_P{pred}_Pr{prob:.2f}.png", x_vis, cmap="gray")

plt.suptitle("Misclassified Failure Cases (Test)")
plt.tight_layout()
plt.savefig("outputs/failure_grid.png", dpi=200)
plt.show()

"""9) Markdown report"""

report_path = Path("task1_classification_report.md")

md = f"""# Task 1 – Pneumonia Classification (PneumoniaMNIST)

## Model Architecture (Chosen: **{MODEL_NAME}**)
- Implemented options: SimpleCNN, ResNet18, EfficientNet-B0, ViT-Tiny.
- Chosen model: **{MODEL_NAME}** (strong baseline for small medical images; pretrained features improve generalization).

## Data Pipeline
- Dataset: PneumoniaMNIST (train/val/test official splits).
- Normalization: mean={ckpt['mean']:.4f}, std={ckpt['std']:.4f} computed from **train** split.
- Augmentation (train only): mild brightness/contrast, small rotation (±7°), small translation (≤2px), mild Gaussian noise.
- No flips (preserve laterality).

## Training Methodology & Hyperparameters
- Loss: CrossEntropyLoss
- Optimizer: AdamW
- LR schedule: CosineAnnealingLR
- Hyperparams: {json.dumps(ckpt['hyperparams'], indent=2)}

## Test Evaluation Metrics (threshold=0.5)
- Accuracy: **{acc:.4f}**
- Precision: **{prec:.4f}**
- Recall: **{rec:.4f}**
- F1-score: **{f1:.4f}**
- AUC: **{auc:.4f}**

## Visualizations
- Training curves:
  - outputs/training_curves_loss.png
  - outputs/training_curves_auc.png
- Confusion matrix:
  - outputs/confusion_matrix.png
- ROC curve:
  - outputs/roc_curve.png

## Failure Case Analysis
- Example misclassified images:
  - outputs/failure_grid.png
  - outputs/failures/ (individual images)
- Possible reasons:
  1. Low resolution (28×28) reduces visibility of subtle opacities.
  2. Borderline / mild pneumonia cases resemble normal patterns.
  3. Threshold=0.5 may not be optimal for sensitivity-specificity tradeoff.
  4. Noise/brightness variation may create ambiguity in weak-signal images.

## Strengths & Limitations
**Strengths**
- End-to-end reproducible pipeline with official splits and train-based normalization.
- Reports complete metric suite + CM + ROC + failure cases.
- Supports strong modern architectures (EfficientNet/ResNet/ViT).

**Limitations**
- PneumoniaMNIST is downsampled; clinical CXR datasets are higher resolution.
- Single threshold reporting; could tune threshold for higher recall in clinical screening.
- No explicit calibration analysis (e.g., reliability curve).

## Reproducibility
- Best checkpoint: **{best_ckpt.as_posix()}**
- To reproduce: run training cells or export scripts below (train.py/eval.py).
"""

report_path.write_text(md, encoding="utf-8")
print("Saved:", report_path)

"""Create train.py and eval.py scripts inside Colab
10A) Write train.py
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile train.py
# import argparse, json, random
# from pathlib import Path
# import numpy as np
# import torch
# import torch.nn as nn
# from torch.utils.data import Dataset, DataLoader
# import torchvision.transforms.functional as TF
# import timm
# from sklearn.metrics import roc_auc_score
# from medmnist import PneumoniaMNIST
# 
# def seed_all(seed: int):
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
# 
# class Pneumo28(Dataset):
#     """
#     PneumoniaMNIST returns PIL grayscale images (28x28).
#     We convert to torch tensor [1,28,28] in [0,1], apply (optional) augmentation,
#     then normalize with train mean/std.
#     """
#     def __init__(self, base_ds, mean: float, std: float, augment: bool = False):
#         self.ds = base_ds
#         self.mean = float(mean)
#         self.std = float(std)
#         self.augment = augment
# 
#     def __len__(self):
#         return len(self.ds)
# 
#     def _augment(self, x: torch.Tensor) -> torch.Tensor:
#         # Brightness/contrast (CXR-safe mild)
#         if random.random() < 0.8:
#             b = random.uniform(0.85, 1.15)
#             c = random.uniform(0.85, 1.15)
#             x = TF.adjust_brightness(x, b)
#             x = TF.adjust_contrast(x, c)
# 
#         # Small rotation (±7°)
#         if random.random() < 0.5:
#             angle = random.uniform(-7, 7)
#             x = TF.rotate(x, angle=angle)
# 
#         # Small translation (≤2 px)
#         if random.random() < 0.5:
#             dx = random.randint(-2, 2)
#             dy = random.randint(-2, 2)
#             x = TF.affine(x, angle=0, translate=[dx, dy], scale=1.0, shear=[0.0, 0.0])
# 
#         # Mild Gaussian noise
#         if random.random() < 0.5:
#             x = torch.clamp(x + 0.02 * torch.randn_like(x), 0.0, 1.0)
# 
#         return x
# 
#     def __getitem__(self, idx):
#         img, label = self.ds[idx]
#         x = torch.from_numpy(np.asarray(img, dtype=np.float32)).unsqueeze(0) / 255.0
#         y = int(np.asarray(label).squeeze())
# 
#         if self.augment:
#             x = self._augment(x)
# 
#         x = (x - self.mean) / (self.std + 1e-8)
#         return x, y
# 
# class SimpleCNN(nn.Module):
#     def __init__(self, num_classes=2):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
#             nn.AdaptiveAvgPool2d(1),
#         )
#         self.head = nn.Linear(128, num_classes)
# 
#     def forward(self, x):
#         x = self.net(x).flatten(1)
#         return self.head(x)
# 
# def build_model(name: str, num_classes: int = 2) -> nn.Module:
#     name = name.lower()
#     if name == "simplecnn":
#         return SimpleCNN(num_classes=num_classes)
#     if name == "resnet18":
#         return timm.create_model("resnet18", pretrained=True, num_classes=num_classes, in_chans=1)
#     if name == "efficientnet_b0":
#         return timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes, in_chans=1)
#     if name == "vit_tiny":
#         return timm.create_model("vit_tiny_patch16_224", pretrained=True, num_classes=num_classes, in_chans=1)
#     raise ValueError(f"Unknown model: {name}")
# 
# def maybe_resize(x: torch.Tensor, model_name: str) -> torch.Tensor:
#     # ViT expects 224x224; resize only for ViT
#     if model_name.lower().startswith("vit"):
#         return torch.nn.functional.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)
#     return x
# 
# def compute_mean_std(ds) -> tuple[float, float]:
#     s1, s2 = 0.0, 0.0
#     n = len(ds)
#     for i in range(n):
#         img, _ = ds[i]
#         arr = np.asarray(img, dtype=np.float32) / 255.0
#         s1 += float(arr.mean())
#         s2 += float((arr ** 2).mean())
#     mean = s1 / n
#     var = (s2 / n) - mean ** 2
#     std = float(np.sqrt(max(var, 1e-12)))
#     return float(mean), float(std)
# 
# @torch.no_grad()
# def eval_auc(model: nn.Module, loader: DataLoader, model_name: str, device: str) -> float:
#     model.eval()
#     probs, ys = [], []
#     for x, y in loader:
#         x = maybe_resize(x.to(device), model_name)
#         logits = model(x)
#         p = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()
#         probs.append(p)
#         ys.append(y.numpy())
#     probs = np.concatenate(probs)
#     ys = np.concatenate(ys)
#     return float(roc_auc_score(ys, probs))
# 
# def main():
#     ap = argparse.ArgumentParser()
#     ap.add_argument("--model", default="efficientnet_b0",
#                     choices=["simplecnn", "resnet18", "efficientnet_b0", "vit_tiny"])
#     ap.add_argument("--epochs", type=int, default=15)
#     ap.add_argument("--batch", type=int, default=256)
#     ap.add_argument("--lr", type=float, default=3e-4)
#     ap.add_argument("--wd", type=float, default=1e-4)
#     ap.add_argument("--seed", type=int, default=42)
#     ap.add_argument("--data_root", default="./data")
#     ap.add_argument("--out_dir", default="./models")
#     ap.add_argument("--num_workers", type=int, default=2)
#     ap.add_argument("--pin_memory", type=int, default=1)
#     args, _ = ap.parse_known_args()  # ✅ fixes Colab/Jupyter "-f kernel.json" issue
# 
#     seed_all(args.seed)
#     device = "cuda" if torch.cuda.is_available() else "cpu"
# 
#     train_raw = PneumoniaMNIST(split="train", download=True, root=args.data_root)
#     val_raw   = PneumoniaMNIST(split="val", download=True, root=args.data_root)
# 
#     mean, std = compute_mean_std(train_raw)
# 
#     train_ds = Pneumo28(train_raw, mean, std, augment=True)
#     val_ds   = Pneumo28(val_raw, mean, std, augment=False)
# 
#     train_loader = DataLoader(
#         train_ds, batch_size=args.batch, shuffle=True,
#         num_workers=args.num_workers, pin_memory=bool(args.pin_memory)
#     )
#     val_loader = DataLoader(
#         val_ds, batch_size=args.batch, shuffle=False,
#         num_workers=args.num_workers, pin_memory=bool(args.pin_memory)
#     )
# 
#     model = build_model(args.model).to(device)
#     criterion = nn.CrossEntropyLoss()
#     optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
# 
#     out_dir = Path(args.out_dir)
#     out_dir.mkdir(parents=True, exist_ok=True)
# 
#     best_auc = -1.0
#     best_weights_path = out_dir / f"best_{args.model}.pt"
#     best_meta_path = out_dir / f"best_{args.model}_meta.json"
# 
#     history = {"train_loss": [], "val_auc": []}
# 
#     for epoch in range(1, args.epochs + 1):
#         model.train()
#         running = 0.0
# 
#         for x, y in train_loader:
#             x = maybe_resize(x.to(device), args.model)
#             y = y.to(device)
# 
#             optimizer.zero_grad(set_to_none=True)
#             logits = model(x)
#             loss = criterion(logits, y)
#             loss.backward()
#             optimizer.step()
# 
#             running += loss.item() * y.size(0)
# 
#         scheduler.step()
# 
#         train_loss = running / len(train_loader.dataset)
#         val_auc = eval_auc(model, val_loader, args.model, device)
# 
#         history["train_loss"].append(float(train_loss))
#         history["val_auc"].append(float(val_auc))
# 
#         print(f"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_auc={val_auc:.4f}")
# 
#         if val_auc > best_auc:
#             best_auc = val_auc
#             # ✅ save weights ONLY (avoids torch.load pickle errors)
#             torch.save(model.state_dict(), best_weights_path)
#             # ✅ save metadata separately as JSON (safe)
#             meta = {
#                 "model_name": args.model,
#                 "mean": mean,
#                 "std": std,
#                 "best_val_auc": best_auc,
#                 "hyperparams": vars(args),
#                 "history": history,
#             }
#             best_meta_path.write_text(json.dumps(meta, indent=2), encoding="utf-8")
# 
#     print("\nSaved best weights:", best_weights_path)
#     print("Saved metadata:", best_meta_path)
# 
# if __name__ == "__main__":
#     main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile eval.py
# import argparse, json
# from pathlib import Path
# import numpy as np
# import torch
# import torch.nn as nn
# from torch.utils.data import Dataset, DataLoader
# import timm
# import matplotlib.pyplot as plt
# from sklearn.metrics import (
#     accuracy_score, precision_score, recall_score, f1_score,
#     roc_auc_score, confusion_matrix, roc_curve
# )
# from medmnist import PneumoniaMNIST
# 
# class SimpleCNN(nn.Module):
#     def __init__(self, num_classes=2):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
#             nn.AdaptiveAvgPool2d(1),
#         )
#         self.head = nn.Linear(128, num_classes)
#     def forward(self, x):
#         x = self.net(x).flatten(1)
#         return self.head(x)
# 
# def build_model(name: str, num_classes: int = 2) -> nn.Module:
#     name = name.lower()
#     if name == "simplecnn":
#         return SimpleCNN(num_classes=num_classes)
#     if name == "resnet18":
#         return timm.create_model("resnet18", pretrained=False, num_classes=num_classes, in_chans=1)
#     if name == "efficientnet_b0":
#         return timm.create_model("efficientnet_b0", pretrained=False, num_classes=num_classes, in_chans=1)
#     if name == "vit_tiny":
#         return timm.create_model("vit_tiny_patch16_224", pretrained=False, num_classes=num_classes, in_chans=1)
#     raise ValueError(f"Unknown model: {name}")
# 
# def maybe_resize(x: torch.Tensor, model_name: str) -> torch.Tensor:
#     if model_name.lower().startswith("vit"):
#         return torch.nn.functional.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)
#     return x
# 
# class Pneumo28(Dataset):
#     def __init__(self, base_ds, mean: float, std: float):
#         self.ds = base_ds
#         self.mean = float(mean)
#         self.std = float(std)
# 
#     def __len__(self):
#         return len(self.ds)
# 
#     def __getitem__(self, idx):
#         img, label = self.ds[idx]
#         x = torch.from_numpy(np.asarray(img, dtype=np.float32)).unsqueeze(0) / 255.0
#         y = int(np.asarray(label).squeeze())
#         x = (x - self.mean) / (self.std + 1e-8)
#         return x, y
# 
# def denorm(x: torch.Tensor, mean: float, std: float) -> torch.Tensor:
#     return (x * (std + 1e-8) + mean).clamp(0, 1)
# 
# @torch.no_grad()
# def predict_probs(model: nn.Module, loader: DataLoader, model_name: str, device: str):
#     model.eval()
#     probs, ys = [], []
#     for x, y in loader:
#         x = maybe_resize(x.to(device), model_name)
#         logits = model(x)
#         p = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()
#         probs.append(p)
#         ys.append(y.numpy())
#     return np.concatenate(probs), np.concatenate(ys)
# 
# def main():
#     ap = argparse.ArgumentParser()
#     ap.add_argument("--weights", required=True, help="Path to .pt state_dict (e.g., models/best_efficientnet_b0.pt)")
#     ap.add_argument("--meta", required=True, help="Path to _meta.json (e.g., models/best_efficientnet_b0_meta.json)")
#     ap.add_argument("--data_root", default="./data")
#     ap.add_argument("--batch", type=int, default=256)
#     ap.add_argument("--out_dir", default="./outputs")
#     ap.add_argument("--threshold", type=float, default=0.5)
#     ap.add_argument("--num_workers", type=int, default=2)
#     ap.add_argument("--pin_memory", type=int, default=1)
#     args, _ = ap.parse_known_args()  # ✅ fixes Colab/Jupyter "-f kernel.json" issue
# 
#     device = "cuda" if torch.cuda.is_available() else "cpu"
# 
#     meta = json.loads(Path(args.meta).read_text(encoding="utf-8"))
#     model_name = meta["model_name"]
#     mean = float(meta["mean"])
#     std = float(meta["std"])
# 
#     test_raw = PneumoniaMNIST(split="test", download=True, root=args.data_root)
#     test_ds = Pneumo28(test_raw, mean, std)
#     test_loader = DataLoader(
#         test_ds, batch_size=args.batch, shuffle=False,
#         num_workers=args.num_workers, pin_memory=bool(args.pin_memory)
#     )
# 
#     model = build_model(model_name).to(device)
#     state = torch.load(args.weights, map_location=device)  # ✅ loads cleanly in PyTorch 2.6+
#     model.load_state_dict(state)
# 
#     probs, y = predict_probs(model, test_loader, model_name, device)
#     pred = (probs >= args.threshold).astype(int)
# 
#     acc = accuracy_score(y, pred)
#     prec = precision_score(y, pred, zero_division=0)
#     rec = recall_score(y, pred, zero_division=0)
#     f1 = f1_score(y, pred, zero_division=0)
#     auc = roc_auc_score(y, probs)
# 
#     out_dir = Path(args.out_dir)
#     out_dir.mkdir(parents=True, exist_ok=True)
# 
#     # Confusion matrix
#     cm = confusion_matrix(y, pred)
#     plt.figure()
#     plt.imshow(cm)
#     plt.title("Confusion Matrix (Test)")
#     plt.xlabel("Predicted")
#     plt.ylabel("True")
#     for i in range(cm.shape[0]):
#         for j in range(cm.shape[1]):
#             plt.text(j, i, str(cm[i, j]), ha="center", va="center")
#     plt.xticks([0, 1], ["Normal(0)", "Pneumonia(1)"])
#     plt.yticks([0, 1], ["Normal(0)", "Pneumonia(1)"])
#     plt.savefig(out_dir / "confusion_matrix.png", dpi=200)
#     plt.close()
# 
#     # ROC
#     fpr, tpr, _ = roc_curve(y, probs)
#     plt.figure()
#     plt.plot(fpr, tpr)
#     plt.plot([0, 1], [0, 1], linestyle="--")
#     plt.title("ROC Curve (Test)")
#     plt.xlabel("False Positive Rate")
#     plt.ylabel("True Positive Rate")
#     plt.savefig(out_dir / "roc_curve.png", dpi=200)
#     plt.close()
# 
#     # Failure cases
#     failures_dir = out_dir / "failures"
#     failures_dir.mkdir(parents=True, exist_ok=True)
# 
#     mis_idx = np.where(pred != y)[0]
#     max_show = min(25, len(mis_idx))
#     pick = mis_idx[:max_show]
# 
#     plt.figure(figsize=(10, 10))
#     for k, idx in enumerate(pick):
#         x, yt = test_ds[int(idx)]
#         x_vis = denorm(x, mean, std).squeeze(0).numpy()
#         pr = float(probs[int(idx)])
#         yp = int(pred[int(idx)])
# 
#         plt.subplot(5, 5, k + 1)
#         plt.imshow(x_vis, cmap="gray")
#         plt.axis("off")
#         plt.title(f"T={yt} P={yp}\nPr={pr:.2f}", fontsize=8)
# 
#         plt.imsave(str(failures_dir / f"failure_{idx}_T{yt}_P{yp}_Pr{pr:.2f}.png"), x_vis, cmap="gray")
# 
#     plt.suptitle("Misclassified Failure Cases (Test)")
#     plt.tight_layout()
#     plt.savefig(out_dir / "failure_grid.png", dpi=200)
#     plt.close()
# 
#     # Save metrics JSON
#     metrics = {
#         "model": model_name,
#         "threshold": args.threshold,
#         "accuracy": float(acc),
#         "precision": float(prec),
#         "recall": float(rec),
#         "f1": float(f1),
#         "auc": float(auc),
#         "confusion_matrix": cm.tolist(),
#         "num_failures": int(len(mis_idx)),
#     }
#     (out_dir / "metrics.json").write_text(json.dumps(metrics, indent=2), encoding="utf-8")
# 
#     print("✅ Test Metrics")
#     print(json.dumps(metrics, indent=2))
#     print("\nSaved outputs to:", out_dir)
# 
# if __name__ == "__main__":
#     main()
#

!python train.py --model efficientnet_b0 --epochs 15 --batch 256 --lr 3e-4 --wd 1e-4 --out_dir models

!python eval.py \
  --weights models/best_efficientnet_b0.pt \
  --meta models/best_efficientnet_b0_meta.json \
  --out_dir outputs \
  --threshold 0.5

!ls -R models
!ls -R outputs

# Commented out IPython magic to ensure Python compatibility.
# %%writefile models_zoo.py
# import torch
# import torch.nn as nn
# import timm
# 
# # -------------------------
# # 1) Basic CNN
# # -------------------------
# class SimpleCNN(nn.Module):
#     def __init__(self, num_classes=2):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),      # 28->14
#             nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),     # 14->7
#             nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),
#             nn.AdaptiveAvgPool2d(1),
#         )
#         self.head = nn.Linear(128, num_classes)
# 
#     def forward(self, x):
#         x = self.net(x).flatten(1)
#         return self.head(x)
# 
# # -------------------------
# # 2) MambaNet (lightweight "Mamba-like" SSM token mixer)
# #    NOTE: This is a practical Mamba-style model without requiring external mamba-ssm.
# #    It treats each pixel as a token, uses 1D conv mixing + gated residual blocks.
# # -------------------------
# class MambaBlock(nn.Module):
#     def __init__(self, d_model: int, d_state: int = 64, dropout: float = 0.1):
#         super().__init__()
#         self.norm = nn.LayerNorm(d_model)
#         self.in_proj = nn.Linear(d_model, 2 * d_model)  # for gating
#         # token mixer (SSM-ish): depthwise conv over sequence (fast, stable)
#         self.dwconv = nn.Conv1d(d_model, d_model, kernel_size=7, padding=3, groups=d_model)
#         self.state = nn.Conv1d(d_model, d_model, kernel_size=1)  # state mixing
#         self.out_proj = nn.Linear(d_model, d_model)
#         self.drop = nn.Dropout(dropout)
# 
#     def forward(self, x):
#         # x: [B, L, D]
#         h = self.norm(x)
#         gate, val = self.in_proj(h).chunk(2, dim=-1)
#         gate = torch.sigmoid(gate)
# 
#         # conv expects [B, D, L]
#         v = val.transpose(1, 2)
#         v = self.dwconv(v)
#         v = self.state(v)
#         v = v.transpose(1, 2)  # back to [B, L, D]
# 
#         y = gate * v
#         y = self.out_proj(y)
#         y = self.drop(y)
#         return x + y
# 
# class MambaNet(nn.Module):
#     def __init__(self, img_size=28, d_model=128, depth=6, num_classes=2, dropout=0.1):
#         super().__init__()
#         self.img_size = img_size
#         self.seq_len = img_size * img_size
# 
#         # patch embedding = each pixel token (grayscale)
#         self.embed = nn.Linear(1, d_model)
#         self.pos = nn.Parameter(torch.zeros(1, self.seq_len, d_model))
# 
#         self.blocks = nn.Sequential(*[MambaBlock(d_model, dropout=dropout) for _ in range(depth)])
#         self.norm = nn.LayerNorm(d_model)
#         self.head = nn.Linear(d_model, num_classes)
# 
#         nn.init.trunc_normal_(self.pos, std=0.02)
# 
#     def forward(self, x):
#         # x: [B,1,H,W] -> tokens [B,L,1]
#         B, C, H, W = x.shape
#         t = x.view(B, 1, H * W).transpose(1, 2)  # [B, L, 1]
#         t = self.embed(t) + self.pos            # [B, L, D]
#         t = self.blocks(t)
#         t = self.norm(t)
#         cls = t.mean(dim=1)
#         return self.head(cls)
# 
# # -------------------------
# # Builder
# # -------------------------
# def build_model(name: str, num_classes=2):
#     name = name.lower()
#     if name == "simplecnn":
#         return SimpleCNN(num_classes=num_classes)
#     if name == "resnet18":
#         return timm.create_model("resnet18", pretrained=True, num_classes=num_classes, in_chans=1)
#     if name == "efficientnet_b0":
#         return timm.create_model("efficientnet_b0", pretrained=True, num_classes=num_classes, in_chans=1)
#     if name == "vit_tiny":
#         return timm.create_model("vit_tiny_patch16_224", pretrained=True, num_classes=num_classes, in_chans=1)
#     if name == "mambanet":
#         return MambaNet(img_size=28, d_model=128, depth=6, num_classes=num_classes, dropout=0.1)
#     raise ValueError(f"Unknown model: {name}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile pipeline.py
# import argparse, json, random
# from pathlib import Path
# import numpy as np
# import torch
# import torch.nn as nn
# from torch.utils.data import Dataset, DataLoader
# import torchvision.transforms.functional as TF
# import matplotlib.pyplot as plt
# from sklearn.metrics import (
#     accuracy_score, precision_score, recall_score, f1_score,
#     roc_auc_score, confusion_matrix, roc_curve
# )
# from medmnist import PneumoniaMNIST
# from models_zoo import build_model
# 
# def seed_all(seed: int):
#     random.seed(seed); np.random.seed(seed)
#     torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
# 
# class Pneumo28(Dataset):
#     def __init__(self, base_ds, mean, std, augment=False):
#         self.ds = base_ds
#         self.mean = float(mean); self.std = float(std)
#         self.augment = augment
# 
#     def __len__(self): return len(self.ds)
# 
#     def _augment(self, x):
#         if random.random() < 0.8:
#             b = random.uniform(0.85, 1.15)
#             c = random.uniform(0.85, 1.15)
#             x = TF.adjust_brightness(x, b)
#             x = TF.adjust_contrast(x, c)
# 
#         if random.random() < 0.5:
#             angle = random.uniform(-7, 7)
#             x = TF.rotate(x, angle=angle)
# 
#         if random.random() < 0.5:
#             dx = random.randint(-2, 2)
#             dy = random.randint(-2, 2)
#             x = TF.affine(x, angle=0, translate=[dx, dy], scale=1.0, shear=[0.0, 0.0])
# 
#         if random.random() < 0.5:
#             x = torch.clamp(x + 0.02 * torch.randn_like(x), 0.0, 1.0)
#         return x
# 
#     def __getitem__(self, idx):
#         img, label = self.ds[idx]
#         x = torch.from_numpy(np.asarray(img, dtype=np.float32)).unsqueeze(0) / 255.0
#         y = int(np.asarray(label).squeeze())
#         if self.augment:
#             x = self._augment(x)
#         x = (x - self.mean) / (self.std + 1e-8)
#         return x, y
# 
# def maybe_resize(x, model_name):
#     # ViT needs 224
#     if model_name.lower().startswith("vit"):
#         return torch.nn.functional.interpolate(x, size=(224,224), mode="bilinear", align_corners=False)
#     return x
# 
# def compute_mean_std(ds):
#     s1=s2=0.0
#     n=len(ds)
#     for i in range(n):
#         img,_ = ds[i]
#         arr = np.asarray(img, dtype=np.float32)/255.0
#         s1 += float(arr.mean())
#         s2 += float((arr**2).mean())
#     mean = s1/n
#     var = (s2/n) - mean**2
#     std = float(np.sqrt(max(var, 1e-12)))
#     return float(mean), float(std)
# 
# @torch.no_grad()
# def predict_probs(model, loader, model_name, device):
#     model.eval()
#     probs, ys = [], []
#     for x,y in loader:
#         x = maybe_resize(x.to(device), model_name)
#         logits = model(x)
#         p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()
#         probs.append(p); ys.append(y.numpy())
#     return np.concatenate(probs), np.concatenate(ys)
# 
# def denorm(x, mean, std):
#     return (x*(std+1e-8)+mean).clamp(0,1)
# 
# def train_one_model(model_name, train_loader, val_loader, mean, std, args, device, out_models):
#     model = build_model(model_name).to(device)
#     crit = nn.CrossEntropyLoss()
#     opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)
#     sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=args.epochs)
# 
#     best_auc = -1.0
#     history = {"train_loss": [], "val_auc": []}
# 
#     weights_path = out_models / f"best_{model_name}.pt"
#     meta_path    = out_models / f"best_{model_name}_meta.json"
# 
#     for ep in range(1, args.epochs+1):
#         model.train()
#         running = 0.0
#         for x,y in train_loader:
#             x = maybe_resize(x.to(device), model_name)
#             y = y.to(device)
#             opt.zero_grad(set_to_none=True)
#             logits = model(x)
#             loss = crit(logits, y)
#             loss.backward()
#             opt.step()
#             running += loss.item() * y.size(0)
#         sch.step()
# 
#         # val AUC
#         val_probs, val_y = predict_probs(model, val_loader, model_name, device)
#         val_auc = float(roc_auc_score(val_y, val_probs))
# 
#         train_loss = running / len(train_loader.dataset)
#         history["train_loss"].append(float(train_loss))
#         history["val_auc"].append(val_auc)
# 
#         print(f"[{model_name}] Epoch {ep:02d} | train_loss={train_loss:.4f} | val_auc={val_auc:.4f}")
# 
#         if val_auc > best_auc:
#             best_auc = val_auc
#             torch.save(model.state_dict(), weights_path)
#             meta = {
#                 "model_name": model_name,
#                 "mean": mean, "std": std,
#                 "best_val_auc": best_auc,
#                 "hyperparams": vars(args),
#                 "history": history
#             }
#             meta_path.write_text(json.dumps(meta, indent=2), encoding="utf-8")
# 
#     return weights_path, meta_path
# 
# def eval_one_model(model_name, weights_path, meta_path, test_ds, test_loader, out_dir, device, threshold=0.5):
#     meta = json.loads(Path(meta_path).read_text(encoding="utf-8"))
#     mean, std = float(meta["mean"]), float(meta["std"])
# 
#     model = build_model(model_name).to(device)
#     state = torch.load(weights_path, map_location=device)
#     model.load_state_dict(state)
# 
#     probs, y = predict_probs(model, test_loader, model_name, device)
#     pred = (probs >= threshold).astype(int)
# 
#     acc  = float(accuracy_score(y, pred))
#     prec = float(precision_score(y, pred, zero_division=0))
#     rec  = float(recall_score(y, pred, zero_division=0))
#     f1   = float(f1_score(y, pred, zero_division=0))
#     auc  = float(roc_auc_score(y, probs))
#     cm   = confusion_matrix(y, pred)
# 
#     model_out = out_dir / model_name
#     model_out.mkdir(parents=True, exist_ok=True)
# 
#     # Confusion matrix
#     plt.figure()
#     plt.imshow(cm)
#     plt.title(f"Confusion Matrix - {model_name}")
#     plt.xlabel("Predicted"); plt.ylabel("True")
#     for i in range(cm.shape[0]):
#         for j in range(cm.shape[1]):
#             plt.text(j, i, str(cm[i,j]), ha="center", va="center")
#     plt.xticks([0,1], ["Normal(0)", "Pneumonia(1)"])
#     plt.yticks([0,1], ["Normal(0)", "Pneumonia(1)"])
#     plt.savefig(model_out / "confusion_matrix.png", dpi=200)
#     plt.close()
# 
#     # ROC
#     fpr, tpr, _ = roc_curve(y, probs)
#     plt.figure()
#     plt.plot(fpr, tpr)
#     plt.plot([0,1],[0,1], linestyle="--")
#     plt.title(f"ROC - {model_name}")
#     plt.xlabel("FPR"); plt.ylabel("TPR")
#     plt.savefig(model_out / "roc_curve.png", dpi=200)
#     plt.close()
# 
#     # Failure cases (grid + individuals)
#     failures_dir = model_out / "failures"
#     failures_dir.mkdir(parents=True, exist_ok=True)
#     mis_idx = np.where(pred != y)[0]
#     pick = mis_idx[:25]
# 
#     plt.figure(figsize=(10,10))
#     for k, idx in enumerate(pick):
#         x, yt = test_ds[int(idx)]
#         x_vis = denorm(x, mean, std).squeeze(0).numpy()
#         pr = float(probs[int(idx)])
#         yp = int(pred[int(idx)])
#         plt.subplot(5,5,k+1)
#         plt.imshow(x_vis, cmap="gray")
#         plt.axis("off")
#         plt.title(f"T={yt} P={yp}\nPr={pr:.2f}", fontsize=8)
#         plt.imsave(str(failures_dir / f"failure_{idx}_T{yt}_P{yp}_Pr{pr:.2f}.png"), x_vis, cmap="gray")
#     plt.suptitle(f"Failure Cases - {model_name}")
#     plt.tight_layout()
#     plt.savefig(model_out / "failure_grid.png", dpi=200)
#     plt.close()
# 
#     metrics = {
#         "model": model_name,
#         "threshold": threshold,
#         "accuracy": acc,
#         "precision": prec,
#         "recall": rec,
#         "f1": f1,
#         "auc": auc,
#         "confusion_matrix": cm.tolist(),
#         "num_failures": int(len(mis_idx))
#     }
#     (model_out / "metrics.json").write_text(json.dumps(metrics, indent=2), encoding="utf-8")
#     return metrics
# 
# def main():
#     ap = argparse.ArgumentParser()
#     ap.add_argument("--models", default="simplecnn,resnet18,efficientnet_b0,vit_tiny,mambanet")
#     ap.add_argument("--epochs", type=int, default=15)
#     ap.add_argument("--batch", type=int, default=256)
#     ap.add_argument("--lr", type=float, default=3e-4)
#     ap.add_argument("--wd", type=float, default=1e-4)
#     ap.add_argument("--seed", type=int, default=42)
#     ap.add_argument("--data_root", default="./data")
#     ap.add_argument("--out_models", default="./models")
#     ap.add_argument("--out_dir", default="./outputs_compare")
#     ap.add_argument("--threshold", type=float, default=0.5)
#     ap.add_argument("--num_workers", type=int, default=2)
#     ap.add_argument("--pin_memory", type=int, default=1)
#     args, _ = ap.parse_known_args()  # ✅ Colab safe
# 
#     seed_all(args.seed)
#     device = "cuda" if torch.cuda.is_available() else "cpu"
# 
#     # Load raw splits
#     train_raw = PneumoniaMNIST(split="train", download=True, root=args.data_root)
#     val_raw   = PneumoniaMNIST(split="val", download=True, root=args.data_root)
#     test_raw  = PneumoniaMNIST(split="test", download=True, root=args.data_root)
# 
#     mean, std = compute_mean_std(train_raw)
# 
#     train_ds = Pneumo28(train_raw, mean, std, augment=True)
#     val_ds   = Pneumo28(val_raw, mean, std, augment=False)
#     test_ds  = Pneumo28(test_raw, mean, std, augment=False)
# 
#     train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True,
#                               num_workers=args.num_workers, pin_memory=bool(args.pin_memory))
#     val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False,
#                             num_workers=args.num_workers, pin_memory=bool(args.pin_memory))
#     test_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False,
#                              num_workers=args.num_workers, pin_memory=bool(args.pin_memory))
# 
#     out_models = Path(args.out_models); out_models.mkdir(parents=True, exist_ok=True)
#     out_dir = Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)
# 
#     models = [m.strip() for m in args.models.split(",") if m.strip()]
#     results = []
# 
#     # Train + Eval each model
#     for m in models:
#         print("\n==============================")
#         print("Training:", m)
#         weights_path, meta_path = train_one_model(m, train_loader, val_loader, mean, std, args, device, out_models)
# 
#         print("Evaluating:", m)
#         metrics = eval_one_model(m, weights_path, meta_path, test_ds, test_loader, out_dir, device, threshold=args.threshold)
#         results.append(metrics)
# 
#     # Save summary
#     summary_path = out_dir / "summary_metrics.json"
#     summary_path.write_text(json.dumps(results, indent=2), encoding="utf-8")
#     print("\nSaved summary:", summary_path)
# 
#     # Print table-like view
#     print("\n=== Comparative Results (Test) ===")
#     for r in sorted(results, key=lambda x: x["auc"], reverse=True):
#         print(f'{r["model"]:14s} | AUC={r["auc"]:.4f} | Acc={r["accuracy"]:.4f} | F1={r["f1"]:.4f} | R={r["recall"]:.4f} | P={r["precision"]:.4f}')
# 
# if __name__ == "__main__":
#     main()
#

!pip -q install medmnist timm scikit-learn matplotlib torchvision
!python pipeline.py --models simplecnn,resnet18,efficientnet_b0,vit_tiny,mambanet --epochs 15 --batch 256 --lr 3e-4 --wd 1e-4 --out_models models --out_dir outputs_compare --threshold 0.5

import json, os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt

OUT_DIR = Path("outputs_compare")
summary_path = OUT_DIR / "summary_metrics.json"
assert summary_path.exists(), f"Not found: {summary_path}. Run pipeline.py first."

results = json.loads(summary_path.read_text(encoding="utf-8"))
df = pd.DataFrame(results)

# Order columns nicely
cols = ["model", "auc", "accuracy", "f1", "recall", "precision", "num_failures", "threshold"]
df = df[cols].sort_values("auc", ascending=False).reset_index(drop=True)

# Save CSV
df.to_csv(OUT_DIR / "comparison_metrics.csv", index=False)

df

import json
import numpy as np
import torch
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from medmnist import PneumoniaMNIST
from torch.utils.data import DataLoader, Dataset

from models_zoo import build_model

device = "cuda" if torch.cuda.is_available() else "cpu"

OUT_DIR = Path("outputs_compare")
MODELS_DIR = Path("models")
summary = json.loads((OUT_DIR/"summary_metrics.json").read_text())

# Load meta from one model to get normalization (all share same train mean/std)
meta0 = json.loads((MODELS_DIR/f"best_{summary[0]['model']}_meta.json").read_text())
mean, std = float(meta0["mean"]), float(meta0["std"])

# Dataset wrapper (same as pipeline)
class Pneumo28(Dataset):
    def __init__(self, base_ds, mean, std):
        self.ds = base_ds
        self.mean = float(mean); self.std = float(std)
    def __len__(self): return len(self.ds)
    def __getitem__(self, idx):
        img, label = self.ds[idx]
        x = torch.from_numpy(np.asarray(img, dtype=np.float32)).unsqueeze(0) / 255.0
        y = int(np.asarray(label).squeeze())
        x = (x - self.mean) / (self.std + 1e-8)
        return x, y

def maybe_resize(x, model_name):
    if model_name.lower().startswith("vit"):
        return torch.nn.functional.interpolate(x, size=(224,224), mode="bilinear", align_corners=False)
    return x

@torch.no_grad()
def predict_probs(model, loader, model_name):
    model.eval()
    probs, ys = [], []
    for x,y in loader:
        x = maybe_resize(x.to(device), model_name)
        logits = model(x)
        p = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()
        probs.append(p); ys.append(y.numpy())
    return np.concatenate(probs), np.concatenate(ys)

# Prepare test loader
test_raw = PneumoniaMNIST(split="test", download=True, root="./data")
test_ds = Pneumo28(test_raw, mean, std)
test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)

plt.figure()
for item in summary:
    mname = item["model"]
    wpath = MODELS_DIR / f"best_{mname}.pt"

    model = build_model(mname).to(device)
    state = torch.load(wpath, map_location=device)
    model.load_state_dict(state)

    probs, y = predict_probs(model, test_loader, mname)
    fpr, tpr, _ = roc_curve(y, probs)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{mname} (AUC={roc_auc:.3f})")

plt.plot([0,1],[0,1], linestyle="--")
plt.title("Overlay ROC Curves (Test Set)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.savefig(OUT_DIR / "overlay_roc.png", dpi=200, bbox_inches="tight")
plt.show()

print("Saved:", OUT_DIR / "overlay_roc.png")

import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

OUT_DIR = Path("outputs_compare")
df = pd.read_csv(OUT_DIR / "comparison_metrics.csv").sort_values("auc", ascending=False)

# AUC bar
plt.figure()
plt.bar(df["model"], df["auc"])
plt.title("AUC Comparison (Test)")
plt.xlabel("Model")
plt.ylabel("AUC")
plt.xticks(rotation=30, ha="right")
plt.savefig(OUT_DIR / "bar_auc.png", dpi=200, bbox_inches="tight")
plt.show()

# F1 bar
plt.figure()
plt.bar(df["model"], df["f1"])
plt.title("F1-score Comparison (Test)")
plt.xlabel("Model")
plt.ylabel("F1-score")
plt.xticks(rotation=30, ha="right")
plt.savefig(OUT_DIR / "bar_f1.png", dpi=200, bbox_inches="tight")
plt.show()

# Accuracy bar
plt.figure()
plt.bar(df["model"], df["accuracy"])
plt.title("Accuracy Comparison (Test)")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.xticks(rotation=30, ha="right")
plt.savefig(OUT_DIR / "bar_accuracy.png", dpi=200, bbox_inches="tight")
plt.show()

print("Saved:",
      OUT_DIR/"bar_auc.png",
      OUT_DIR/"bar_f1.png",
      OUT_DIR/"bar_accuracy.png")

import json
import pandas as pd
from pathlib import Path

OUT_DIR = Path("outputs_compare")
MODELS_DIR = Path("models")

df = pd.read_csv(OUT_DIR / "comparison_metrics.csv").sort_values("auc", ascending=False)

# Load common training setup details from the best model meta
best_model = df.iloc[0]["model"]
best_meta = json.loads((MODELS_DIR / f"best_{best_model}_meta.json").read_text(encoding="utf-8"))

report = []
report.append("# Task 1 – Pneumonia Detection: Comparative Study (5 Models)\n")
report.append("This report compares five architectures on PneumoniaMNIST using the same preprocessing, augmentation, and evaluation protocol.\n")

report.append("## Models Compared\n")
report.append("- Basic CNN (SimpleCNN)\n")
report.append("- ResNet18\n")
report.append("- EfficientNet-B0\n")
report.append("- ViT-Tiny (patch16_224)\n")
report.append("- MambaNet (SSM-inspired token mixer)\n")

report.append("## Common Data Pipeline\n")
report.append(f"- Dataset: PneumoniaMNIST official splits (train/val/test)\n")
report.append(f"- Normalization: mean={best_meta['mean']:.4f}, std={best_meta['std']:.4f} computed from the train split\n")
report.append("- Augmentation (train only): mild brightness/contrast, rotation (±7°), translation (≤2px), mild Gaussian noise\n")
report.append("- No flips (preserves CXR laterality)\n")

report.append("## Common Training Methodology\n")
hp = best_meta["hyperparams"]
report.append(f"- Loss: CrossEntropyLoss\n")
report.append(f"- Optimizer: AdamW (lr={hp['lr']}, weight_decay={hp['wd']})\n")
report.append(f"- Scheduler: CosineAnnealingLR (epochs={hp['epochs']})\n")
report.append(f"- Batch size: {hp['batch']}\n")
report.append(f"- Selection: best checkpoint by validation AUC\n")

report.append("## Quantitative Results (Test)\n")
report.append("### Summary Table\n")
report.append("![](outputs_compare/comparison_table.png)\n")
report.append("- Full CSV: `outputs_compare/comparison_metrics.csv`\n")

report.append("### ROC Overlay\n")
report.append("![](outputs_compare/overlay_roc.png)\n")

report.append("### Metric Comparison Charts\n")
report.append("![](outputs_compare/bar_auc.png)\n")
report.append("![](outputs_compare/bar_f1.png)\n")
report.append("![](outputs_compare/bar_accuracy.png)\n")

report.append("## Per-model Detailed Outputs\n")
for _, row in df.iterrows():
    m = row["model"]
    report.append(f"### {m}\n")
    report.append(f"- Metrics JSON: `outputs_compare/{m}/metrics.json`\n")
    report.append(f"- Confusion matrix: ![](outputs_compare/{m}/confusion_matrix.png)\n")
    report.append(f"- ROC curve: ![](outputs_compare/{m}/roc_curve.png)\n")
    report.append(f"- Failure grid: ![](outputs_compare/{m}/failure_grid.png)\n")
    report.append(f"- Failure images folder: `outputs_compare/{m}/failures/`\n")

report.append("## Failure Case Discussion (General)\n")
report.append("- Many errors are due to the extremely low resolution (28×28), where subtle opacities are hard to distinguish.\n")
report.append("- Borderline / mild pneumonia patterns can resemble normal radiographs after downsampling.\n")
report.append("- A fixed threshold=0.5 may not be optimal; a sensitivity-focused threshold could reduce false negatives at the cost of more false positives.\n")
report.append("- Some failures may reflect dataset ambiguity or weak signal-to-noise after normalization/augmentation.\n")

report.append("## Strengths and Limitations\n")
report.append("**Strengths**\n")
report.append("- Fully reproducible pipeline with consistent preprocessing across all models.\n")
report.append("- Complete metric suite + ROC/CM + failure cases for error analysis.\n")
report.append("- Covers CNNs, transformer (ViT), and SSM-inspired Mamba-style architecture.\n\n")
report.append("**Limitations**\n")
report.append("- PneumoniaMNIST is downsampled; performance may not translate directly to clinical high-res CXR datasets.\n")
report.append("- No probability calibration analysis (e.g., reliability curve).\n")
report.append("- Threshold optimization not included by default (can be added).\n")

out_path = Path("task1_comparative_report.md")
out_path.write_text("\n".join(report), encoding="utf-8")
print("Saved:", out_path)

!zip -r task1_github_files.zip \
  train.py eval.py pipeline.py models_zoo.py \
  task1_comparative_report.md \
  outputs_compare

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# torch>=2.0
# torchvision
# timm
# medmnist
# scikit-learn
# matplotlib
# pandas
# numpy
#

!ls outputs_compare
!ls outputs_compare/efficientnet_b0

!zip -r model_plots.zip outputs_compare

"""# Task 2: Medical Report Generation using Visual Language Model"""

import gc, torch # Cleanup GPU memory
gc.collect()
torch.cuda.empty_cache()

!pip -q install timm transformers accelerate pillow medmnist scikit-learn matplotlib

#Load ViT-Tiny weights + meta (mean/std)
import json
from pathlib import Path
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image

from models_zoo import build_model  # your Task-1 file

device = "cuda" if torch.cuda.is_available() else "cpu"

VIT_NAME = "vit_tiny"
WEIGHTS_PATH = Path(f"models/best_{VIT_NAME}.pt")
META_PATH    = Path(f"models/best_{VIT_NAME}_meta.json")

assert WEIGHTS_PATH.exists(), f"Missing: {WEIGHTS_PATH}"
assert META_PATH.exists(), f"Missing: {META_PATH}"

meta = json.loads(META_PATH.read_text(encoding="utf-8"))
MEAN = float(meta["mean"])
STD  = float(meta["std"])

vit = build_model(VIT_NAME).to(device)
vit.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device))
vit.eval()

for p in vit.parameters():
    p.requires_grad = False

print("✅ Loaded ViT-Tiny checkpoint")
print("Normalization:", MEAN, STD)

# Test dataset + preprocessing (matches Task-1)
from medmnist import PneumoniaMNIST

test_ds = PneumoniaMNIST(split="test", download=True)

def to_pil(img):
    return Image.fromarray(np.array(img))

def preprocess_for_vit(pil_img, mean=MEAN, std=STD):
    """
    Task-1 used: tensor in [0,1] from 28x28, normalize using train mean/std,
    then resize to 224x224 for ViT.
    """
    x = torch.from_numpy(np.asarray(pil_img, dtype=np.float32)).unsqueeze(0) / 255.0  # [1,28,28]
    x = (x - mean) / (std + 1e-8)                                                     # normalize
    x = x.unsqueeze(0)                                                                # [1,1,28,28]
    x = F.interpolate(x, size=(224, 224), mode="bilinear", align_corners=False)       # ViT input
    return x

#ViT probability + prediction
@torch.no_grad()
def vit_prob_and_pred(pil_img):
    x = preprocess_for_vit(pil_img).to(device)
    logits = vit(x)
    prob = torch.softmax(logits, dim=1)[:, 1].item()  # pneumonia class prob
    pred = 1 if prob >= 0.5 else 0
    return prob, pred

!pip -q install -U transformers accelerate huggingface_hub timm medmnist scikit-learn pillow

from huggingface_hub import login
login()  # paste your HF token (READ token)

import json
from pathlib import Path
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image

# from Task-1 file
from models_zoo import build_model

device = "cuda" if torch.cuda.is_available() else "cpu"

VIT_NAME = "vit_tiny"
WEIGHTS_PATH = Path(f"models/best_{VIT_NAME}.pt")
META_PATH    = Path(f"models/best_{VIT_NAME}_meta.json")

assert WEIGHTS_PATH.exists(), f"Missing: {WEIGHTS_PATH}"
assert META_PATH.exists(), f"Missing: {META_PATH}"

meta = json.loads(META_PATH.read_text(encoding="utf-8"))
MEAN = float(meta["mean"])
STD  = float(meta["std"])

vit = build_model(VIT_NAME).to(device)
vit.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device))
vit.eval()

for p in vit.parameters():
    p.requires_grad = False

print("✅ ViT loaded | mean/std:", MEAN, STD)

from medmnist import PneumoniaMNIST

test_ds = PneumoniaMNIST(split="test", download=True)

def to_pil(img):
    return Image.fromarray(np.array(img))

def preprocess_for_vit(pil_img, mean=MEAN, std=STD):
    # 28x28 -> normalize -> resize 224 for ViT
    x = torch.from_numpy(np.asarray(pil_img, dtype=np.float32)).unsqueeze(0) / 255.0  # [1,28,28]
    x = (x - mean) / (std + 1e-8)
    x = x.unsqueeze(0)  # [1,1,28,28]
    x = F.interpolate(x, size=(224,224), mode="bilinear", align_corners=False)
    return x

@torch.no_grad()
def vit_prob_pred(pil_img, thr=0.5):
    x = preprocess_for_vit(pil_img).to(device)
    logits = vit(x)
    prob = torch.softmax(logits, dim=1)[:, 1].item()
    pred = int(prob >= thr)
    return prob, pred

# Compute ViT probs/preds on whole test set
probs, gts, preds = [], [], []
for i in range(len(test_ds)):
    img, gt = test_ds[i]
    pil = to_pil(img)
    p, pr = vit_prob_pred(pil)
    probs.append(p)
    gts.append(int(np.asarray(gt).squeeze()))
    preds.append(pr)

probs = np.array(probs)
gts   = np.array(gts)
preds = np.array(preds)

correct_normal = np.where((gts==0) & (preds==0))[0]
correct_pneu   = np.where((gts==1) & (preds==1))[0]
miscls         = np.where(preds!=gts)[0]
uncertain      = np.argsort(np.abs(probs-0.5))  # closest to 0.5 first

pick = []
pick += list(correct_normal[:3])
pick += list(correct_pneu[:3])
pick += list(miscls[:2]) if len(miscls)>=2 else list(miscls)
# add uncertain but avoid duplicates
for idx in uncertain:
    if idx not in pick:
        pick.append(int(idx))
    if len(pick) >= 10:
        break

pick = pick[:10]
pick

!pip install -U transformers

from huggingface_hub import logout, login, whoami

logout()
login()  # paste the NEW "Read" token

from huggingface_hub import whoami
#print(whoami())

import torch
from transformers import pipeline

pipe = pipeline(
    "image-text-to-text",
    model="google/medgemma-1.5-4b-it",
    torch_dtype=torch.bfloat16,
    device="cuda" if torch.cuda.is_available() else "cpu",
)

print("✅ MedGemma loaded successfully")

from PIL import Image
import numpy as np

PROMPTS = {
  "P1_concise": (
    "You are a radiology assistant. Write a concise chest X-ray impression. "
    "State whether findings are consistent with pneumonia or not."
  ),
  "P2_structured": (
    "You are a radiology assistant. Provide:\n"
    "1) Findings (short bullet points)\n"
    "2) Impression (1 sentence)\n"
    "Focus on pneumonia-related signs if present."
  ),
  "P3_cautious": (
    "You are a radiology assistant. Describe visible abnormalities conservatively. "
    "If pneumonia is uncertain, say 'cannot exclude pneumonia' rather than making a definite claim. "
    "Keep it short."
  ),
}

def medgemma_generate(pil_img, prompt, max_new_tokens=256):
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": pil_img},
            {"type": "text", "text": prompt}
        ]
    }]
    out = pipe(text=messages, max_new_tokens=max_new_tokens, do_sample=False)
    return out[0]["generated_text"][-1]["content"]

import numpy as np
from medmnist import PneumoniaMNIST
from PIL import Image

# Load test set
test_ds = PneumoniaMNIST(split="test", download=True)

def to_pil(img):
    return Image.fromarray(np.array(img))

# If you already have probs/gts/preds from earlier, skip this block.
probs, gts, preds = [], [], []
for i in range(len(test_ds)):
    img, gt = test_ds[i]
    pil = to_pil(img)
    p, pr = vit_prob_pred(pil)   # uses your ViT from Task-1
    probs.append(p)
    gts.append(int(np.asarray(gt).squeeze()))
    preds.append(pr)

probs = np.array(probs); gts = np.array(gts); preds = np.array(preds)

correct_normal = np.where((gts==0) & (preds==0))[0]
correct_pneu   = np.where((gts==1) & (preds==1))[0]
miscls         = np.where(preds!=gts)[0]
uncertain      = np.argsort(np.abs(probs-0.5))

pick = []
pick += list(correct_normal[:3])
pick += list(correct_pneu[:3])
pick += list(miscls[:2]) if len(miscls) >= 2 else list(miscls)

for idx in uncertain:
    if idx not in pick:
        pick.append(int(idx))
    if len(pick) >= 10:
        break

pick = pick[:10]
pick

import json
from pathlib import Path

OUT = Path("task2_outputs_medgemma")
(OUT / "images").mkdir(parents=True, exist_ok=True)

results = []
for k, idx in enumerate(pick):
    img, gt = test_ds[int(idx)]
    pil = to_pil(img)

    # Save image
    gt_int = int(np.asarray(gt).squeeze())
    img_path = OUT / "images" / f"img_{k:02d}_idx{idx}_gt{gt_int}.png"
    pil.save(img_path)

    vit_p, vit_pr = vit_prob_pred(pil)

    item = {
        "sample_id": k,
        "dataset_index": int(idx),
        "ground_truth": gt_int,
        "vit_prob": float(vit_p),
        "vit_pred": int(vit_pr),
        "image_path": str(img_path),
        "medgemma_outputs": {}
    }

    for pname, prompt in PROMPTS.items():
        item["medgemma_outputs"][pname] = medgemma_generate(pil, prompt, max_new_tokens=256)

    results.append(item)

json_path = OUT / "task2_samples.json"
json_path.write_text(json.dumps(results, indent=2), encoding="utf-8")

print("✅ Saved JSON:", json_path)
print("✅ Saved images in:", OUT / "images")

def mentions_no_pneumonia(text):
    t = text.lower()
    return ("no pneumonia" in t) or ("no evidence of pneumonia" in t) or ("not consistent with pneumonia" in t)

def mentions_pneumonia(text):
    t = text.lower()
    return ("pneumonia" in t) and not mentions_no_pneumonia(t)

summary = {p: {"match_gt": 0, "contradict_gt": 0} for p in PROMPTS}

for r in results:
    gt = r["ground_truth"]
    for pname in PROMPTS:
        txt = r["medgemma_outputs"][pname]
        # rough rule-based alignment:
        if gt == 1:
            ok = mentions_pneumonia(txt) and not mentions_no_pneumonia(txt)
        else:
            ok = mentions_no_pneumonia(txt) or (not mentions_pneumonia(txt))
        if ok:
            summary[pname]["match_gt"] += 1
        else:
            summary[pname]["contradict_gt"] += 1

summary

from pathlib import Path

md = []
md.append("# Task 2: Medical Report Generation using Visual Language Model (MedGemma)\n")

md.append("## Model Selection Justification\n")
md.append("- **Selected model:** `google/medgemma-1.5-4b-it` (MedGemma 1.5, multimodal instruction-tuned).\n")
md.append("- **Justification:** MedGemma is a medical-focused VLM designed for clinically relevant image understanding and report-style generation, making it more suitable than general captioning models for chest X-ray interpretation.\n")

md.append("## Pipeline Overview\n")
md.append("- Input: Chest X-ray image (PneumoniaMNIST test split)\n")
md.append("- Reference classifier: **ViT-Tiny** (best Task-1 model) provides pneumonia probability/prediction\n")
md.append("- VLM generation: MedGemma generates short impressions using prompt instructions\n")
md.append("- Outputs: JSON + saved images + qualitative analysis\n")

md.append("## Prompting Strategies Tested\n")
for pname, ptext in PROMPTS.items():
    md.append(f"### {pname}\n")
    md.append(f"{ptext}\n")

md.append("## Prompt Effectiveness (Qualitative Summary)\n")
for pname in PROMPTS:
    md.append(f"- **{pname}:** match_gt={summary[pname]['match_gt']}, contradict_gt={summary[pname]['contradict_gt']} (rule-based screening)\n")

md.append("\n## Sample Generated Reports (10 images)\n")
md.append("Each sample shows the image, Ground Truth label, ViT prediction, and MedGemma outputs.\n")

for r in results:
    md.append(f"---\n### Sample {r['sample_id']} (dataset idx={r['dataset_index']})\n")
    md.append(f"- **Ground Truth:** {r['ground_truth']} (0=Normal, 1=Pneumonia)\n")
    md.append(f"- **ViT Prediction:** {r['vit_pred']} (p={r['vit_prob']:.2f})\n")
    md.append(f"- **Image:** ![]({r['image_path']})\n")
    for pname in PROMPTS:
        md.append(f"**{pname} Output:**\n\n{r['medgemma_outputs'][pname]}\n")

md.append("## Qualitative Analysis: VLM vs Ground Truth vs CNN\n")
md.append("- We evaluated outputs on a mixed set of normal, pneumonia, misclassified, and uncertain cases.\n")
md.append("- In many cases, MedGemma produced clinically styled impressions; structured prompting (P2) typically encouraged more organized responses.\n")
md.append("- Disagreements often occurred on ambiguous/low-resolution cases (PneumoniaMNIST images are 28×28), where subtle opacities are difficult to confirm.\n")
md.append("- For misclassified images, we compared whether MedGemma’s description aligned more closely with the ground truth or the ViT prediction.\n")

md.append("## Strengths and Limitations\n")
md.append("**Strengths**\n")
md.append("- Medical-domain VLM, capable of radiology-like phrasing.\n")
md.append("- Prompting significantly affects structure and caution of outputs.\n\n")
md.append("**Limitations**\n")
md.append("- PneumoniaMNIST is very low resolution; real clinical CXR are higher quality.\n")
md.append("- VLM outputs are not guaranteed to be clinically correct and may hallucinate; should not be used for real diagnosis.\n")
md.append("- Evaluation is qualitative; no ground-truth report text exists for scoring.\n")

out_md = Path("task2 report generation.md")
out_md.write_text("\n".join(md), encoding="utf-8")
print("✅ Saved:", out_md)